{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#데이터셋\n",
    "DATASET_NAME = 'kfood'\n",
    "DRIVE_PATH = Path(os.getcwd())\n",
    "DATASET_PATH = DRIVE_PATH / DATASET_NAME\n",
    "filepath = DATASET_PATH\n",
    "print(filepath.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_label 매칭 딕셔너리로 저장\n",
    "labels = None\n",
    "classes = None\n",
    "with open(filepath / 'class_label.csv','r') as f:\n",
    "    w = csv.reader(f)\n",
    "    classes = w.__next__()\n",
    "    labels = w.__next__()\n",
    "#print(len(classes), len(labels), classes, labels)\n",
    "class_to_label = {}\n",
    "for _class, _label in zip(classes, labels):\n",
    "    class_to_label[_class] = int(_label)\n",
    "tf_class_to_label = tf.constant(list(class_to_label.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_crop_points(filepath):\n",
    "    crops = {}\n",
    "    properties = filepath / \"crop_area.properties\"\n",
    "    with open(properties, 'r') as p:\n",
    "        for row in p:\n",
    "            name, crop = row.replace(\"\\n\", \"\").replace(\" \", \"\").split(\"=\")\n",
    "            if name != \"\" and crop != \"\":\n",
    "                #name = name.encode('utf-8')\n",
    "                crop = crop.split(\",\")\n",
    "                if len(crop) >= 4:\n",
    "                    crop = [int(crop[1]), int(crop[0]), int(crop[3]), int(crop[2])]\n",
    "                    crops[name] = crop\n",
    "                elif len(crop) == 2:\n",
    "                    crop = [0, 0, int(crop[1]), int(crop[0])]\n",
    "                    crops[name] = crop\n",
    "            \n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop 지점 정보 빼오기\n",
    "crop_points = {}\n",
    "class_list = list(filepath.glob(\"*/*\"))\n",
    "class_list = [class_name for class_name in class_list if class_name.is_dir()]\n",
    "for class_name in class_list:\n",
    "    crop_points.update(get_image_crop_points(class_name))\n",
    "\n",
    "tf_crop_image_names = tf.constant(list(crop_points.keys()), dtype=tf.string)\n",
    "tf_crop_points = tf.constant(list(crop_points.values()), dtype=tf.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150507\n"
     ]
    }
   ],
   "source": [
    "#데이터셋의 이미지 경로 및 레이블 저장\n",
    "from glob import glob\n",
    "image_paths = sorted(glob(\"kfood/*/*/*\"))\n",
    "image_paths = [image_path for image_path in image_paths if image_path.split(\"/\")[-1].split(\".\")[-1].lower() in (\"png\", \"jpg\", \"jpeg\")]\n",
    "#labels = [class_to_label[Path(image_path).parent.stem] for image_path in image_paths]\n",
    "print(len(image_paths))#, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_crop_image_add_label(tf_filepath):\n",
    "    \n",
    "    image = tf.io.read_file(tf_filepath) # 이미지 파일 읽기\n",
    "    filepath = tf.compat.path_to_str(tf_filepath)\n",
    "    #format decoding\n",
    "    image_format = tf.strings.split(filepath, \".\")[-1]\n",
    "    if image_format == \"jpeg\":\n",
    "        image = tf.image.decode_jpeg(image, channels=3) # JPEG-encoded -> uint8 tensor (RGB format)\n",
    "    elif image_format == 'png':\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "    else:\n",
    "        image = tf.image.decode_image(image, channels=3)\n",
    "    \n",
    "    #crop\n",
    "    image_name = tf.strings.split(tf.strings.split(filepath, \"/\")[-1], \".\")[0]\n",
    "    tf_image_idx = tf.where(tf_crop_image_names == image_name)\n",
    "    try:\n",
    "        image = tf.image.crop_to_bounding_box(image, *tf_crop_points[tf.reshape(tf_image_idx, shape=())])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #labeling\n",
    "    class_name = tf.strings.split(filepath, \"/\")[-2]\n",
    "    tf_class_name_idx = tf.where(tf_class_to_label == class_name)\n",
    "    try:\n",
    "        label = tf.reshape(tf_class_name_idx, shape=())\n",
    "    except:\n",
    "        label = 0\n",
    "        print(\"label error\")\n",
    "\n",
    "    return image, int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kfood_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None, n_parse_threads=5, batch_size=32, cache=False):\n",
    "\n",
    "    filenames_dataset = tf.data.Dataset.from_tensor_slices(filepaths)\n",
    "    dataset = filenames_dataset.map(parse_and_crop_image_add_label, num_parallel_calls=n_parse_threads)\n",
    "    #dataset = filenames_dataset.map(spa)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    if batch_size:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 생성\n",
    "dataset = make_kfood_dataset(image_paths, shuffle_buffer_size=10000, n_parse_threads=tf.data.AUTOTUNE, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "crop_points = {}\n",
    "class_list = list(filepath.glob(\"*/*\"))\n",
    "print(len(class_list))\n",
    "class_list = [class_name for class_name in class_list if class_name.is_dir()]\n",
    "print(len(class_list))\n",
    "for class_name in class_list:\n",
    "    crop_points.update(get_image_crop_points(class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51660"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{ image_filename : crop_point, ... }\n",
    "len(crop_points.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_and_crop_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ty/sj9j6sl158j5bv97y8pxd76c0000gn/T/ipykernel_15944/1534784126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_and_crop_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_and_crop_image' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "tf.shape(parse_and_crop_image(image_paths[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "images_ds = filenames_dataset.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([], shape=(0,), dtype=int32)\n",
      "tf.Tensor([400 534   3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 13:16:07.492231: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-24 13:16:07.492404: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ty/sj9j6sl158j5bv97y8pxd76c0000gn/T/ipykernel_94435/3524824257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtfrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_single_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example_v2\u001b[0;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0mserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"serialized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0mserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_assert_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serialized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparse_example_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[0;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[1;32m    312\u001b[0m   ])\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_example_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_construct_tensors_for_composite_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_parse_example_raw\u001b[0;34m(serialized, names, params, name)\u001b[0m\n\u001b[1;32m    348\u001b[0m       raise ValueError(\"serialized must have statically-known rank to \"\n\u001b[1;32m    349\u001b[0m                        \"parse ragged features.\")\n\u001b[0;32m--> 350\u001b[0;31m     outputs = gen_parsing_ops.parse_example_v2(\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[0;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name)\u001b[0m\n\u001b[1;32m    716\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m       return parse_example_v2_eager_fallback(\n\u001b[0m\u001b[1;32m    719\u001b[0m           \u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mragged_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m           \u001b[0mdense_defaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2_eager_fallback\u001b[0;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name, ctx)\u001b[0m\n\u001b[1;32m    810\u001b[0m   \u001b[0msparse_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ragged_value_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mragged_value_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   \"ragged_split_types\", ragged_split_types, \"dense_shapes\", dense_shapes)\n\u001b[0;32m--> 812\u001b[0;31m   _result = _execute.execute(b\"ParseExampleV2\", num_sparse + len(sparse_types)\n\u001b[0m\u001b[1;32m    813\u001b[0m                              \u001b[0;34m+\u001b[0m \u001b[0mnum_sparse\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_defaults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mragged_value_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf25/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_descriptions = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=0)\n",
    "}\n",
    "for tfrecord in dataset.take(1):\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2cd6330df87933e268d4b1d5797575da50d54717d7327a2d2e545dc9a1c6c69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf25': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
